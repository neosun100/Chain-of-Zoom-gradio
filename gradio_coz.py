import os
import sys
sys.path.append(os.getcwd())
import glob
import torch
from torchvision import transforms
import numpy as np
from PIL import Image
import gradio as gr
import time
import uuid

# å¯¼å…¥åŸæœ‰ä»£ç ä¸­çš„æ‰€æœ‰å‡½æ•°
from ram.models.ram_lora import ram
from ram import inference_ram as inference
from utils.wavelet_color_fix import adain_color_fix, wavelet_color_fix

# ä¿ç•™åŸå§‹ä»£ç ä¸­çš„è¾…åŠ©å‡½æ•°
tensor_transforms = transforms.Compose([transforms.ToTensor()])
ram_transforms = transforms.Compose([
    transforms.Resize((384, 384)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def resize_and_center_crop(img: Image.Image, size: int) -> Image.Image:
    w, h = img.size
    scale = size / min(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    img = img.resize((new_w, new_h), Image.LANCZOS)
    left = (new_w - size) // 2
    top  = (new_h - size) // 2
    return img.crop((left, top, left + size, top + size))

# ä»åŸå§‹ä»£ç ä¸­ä¿ç•™get_validation_promptå‡½æ•°
def get_validation_prompt(args, image, prompt_image_path, dape_model=None, vlm_model=None, device='cuda'):
    # å‡†å¤‡ä½åˆ†è¾¨ç‡å¼ é‡ä½œä¸ºSRè¾“å…¥
    lq = tensor_transforms(image).unsqueeze(0).to(device)
    # é€‰æ‹©æç¤ºæº
    if args.prompt_type == "null":
        prompt_text = args.prompt or ""
    elif args.prompt_type == "dape":
        lq_ram = ram_transforms(lq).to(dtype=weight_dtype)
        captions = inference(lq_ram, dape_model)
        prompt_text = f"{captions[0]}, {args.prompt}," if args.prompt else captions[0]
    elif args.prompt_type in ("vlm"):
        message_text = None
        
        if args.rec_type == "recursive":
            message_text = "What is in this image? Give me a set of words."
            print(f'MESSAGE TEXT: {message_text}')
            messages = [
                {"role": "system", "content": f"{message_text}"},
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": prompt_image_path}
                    ]
                }
            ]
            text = vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = vlm_processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            
        elif args.rec_type == "recursive_multiscale":
            start_image_path = prompt_image_path[0]
            input_image_path = prompt_image_path[1]
            message_text = "The second image is a zoom-in of the first image. Based on this knowledge, what is in the second image? Give me a set of words."
            print(f'START IMAGE PATH: {start_image_path}\nINPUT IMAGE PATH: {input_image_path}\nMESSAGE TEXT: {message_text}')
            messages = [
                {"role": "system", "content": f"{message_text}"},
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": start_image_path},
                        {"type": "image", "image": input_image_path}
                    ]
                }
            ]
            print(f'MESSAGES\n{messages}')

            text = vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = vlm_processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )

        else:
            raise ValueError(f"VLM prompt generation not implemented for rec_type: {args.rec_type}")

        inputs = inputs.to("cuda")

        original_sr_devices = {}
        if args.efficient_memory and 'model' in globals() and hasattr(model, 'text_enc_1'): # Check if SR model is defined
            print("Moving SR model components to CPU for VLM inference.")
            original_sr_devices['text_enc_1'] = model.text_enc_1.device
            original_sr_devices['text_enc_2'] = model.text_enc_2.device
            original_sr_devices['text_enc_3'] = model.text_enc_3.device
            original_sr_devices['transformer'] = model.transformer.device
            original_sr_devices['vae'] = model.vae.device
            
            model.text_enc_1.to('cpu')
            model.text_enc_2.to('cpu')
            model.text_enc_3.to('cpu')
            model.transformer.to('cpu')
            model.vae.to('cpu')
            vlm_model.to('cuda') # vlm_model should already be on its device_map="auto" device

        generated_ids = vlm_model.generate(**inputs, max_new_tokens=128)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = vlm_processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )

        prompt_text = f"{output_text[0]}, {args.prompt}," if args.prompt else output_text[0]

        if args.efficient_memory and 'model' in globals() and hasattr(model, 'text_enc_1'):
            print("Restoring SR model components to original devices.")
            vlm_model.to('cpu') # If vlm_model was moved to a specific cuda device and needs to be offloaded
            model.text_enc_1.to(original_sr_devices['text_enc_1'])
            model.text_enc_2.to(original_sr_devices['text_enc_2'])
            model.text_enc_3.to(original_sr_devices['text_enc_3'])
            model.transformer.to(original_sr_devices['transformer'])
            model.vae.to(original_sr_devices['vae'])
    else:
        raise ValueError(f"Unknown prompt_type: {args.prompt_type}")
    return prompt_text, lq

# åˆ›å»ºå¢å¼ºå‹æ‹¼æ¥å›¾åƒå‡½æ•°
def create_enhanced_concat(images, layout="horizontal", spacing=5, background_color=(240, 240, 240)):
    """
    åˆ›å»ºå¢å¼ºå‹æ‹¼æ¥å›¾åƒï¼Œæ”¯æŒæ°´å¹³ã€å‚ç›´æˆ–ç½‘æ ¼å¸ƒå±€
    
    Args:
        images: å›¾åƒåˆ—è¡¨
        layout: å¸ƒå±€æ–¹å¼ï¼Œå¯é€‰ "horizontal"ã€"vertical" æˆ– "grid"
        spacing: å›¾åƒé—´è·
        background_color: èƒŒæ™¯é¢œè‰²
    
    Returns:
        æ‹¼æ¥åçš„å›¾åƒ
    """
    if not images:
        return None
        
    if layout == "horizontal":
        # æ°´å¹³æ‹¼æ¥
        total_width = sum(img.width for img in images) + spacing * (len(images) - 1)
        max_height = max(img.height for img in images)
        
        concat = Image.new('RGB', (total_width, max_height), background_color)
        x_offset = 0
        
        for img in images:
            concat.paste(img, (x_offset, (max_height - img.height) // 2))
            x_offset += img.width + spacing
            
    elif layout == "vertical":
        # å‚ç›´æ‹¼æ¥
        max_width = max(img.width for img in images)
        total_height = sum(img.height for img in images) + spacing * (len(images) - 1)
        
        concat = Image.new('RGB', (max_width, total_height), background_color)
        y_offset = 0
        
        for img in images:
            concat.paste(img, ((max_width - img.width) // 2, y_offset))
            y_offset += img.height + spacing
            
    elif layout == "grid":
        # ç½‘æ ¼å¸ƒå±€ï¼ˆå°è¯•åˆ›å»ºæ¥è¿‘æ­£æ–¹å½¢çš„å¸ƒå±€ï¼‰
        n = len(images)
        cols = int(np.ceil(np.sqrt(n)))
        rows = int(np.ceil(n / cols))
        
        # è®¡ç®—æ¯ä¸ªå›¾åƒçš„ç›®æ ‡å¤§å°ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        base_size = min(images[0].width, images[0].height)
        scaled_images = []
        
        for img in images:
            # ä¿æŒå®½é«˜æ¯”çš„ç¼©æ”¾
            ratio = base_size / min(img.width, img.height)
            new_size = (int(img.width * ratio), int(img.height * ratio))
            scaled_images.append(img.resize(new_size, Image.LANCZOS))
        
        cell_width = max(img.width for img in scaled_images) + spacing
        cell_height = max(img.height for img in scaled_images) + spacing
        
        grid_width = cols * cell_width - spacing
        grid_height = rows * cell_height - spacing
        
        concat = Image.new('RGB', (grid_width, grid_height), background_color)
        
        for idx, img in enumerate(scaled_images):
            row = idx // cols
            col = idx % cols
            
            x = col * cell_width + (cell_width - img.width) // 2
            y = row * cell_height + (cell_height - img.height) // 2
            
            concat.paste(img, (x, y))
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¸ƒå±€ç±»å‹: {layout}")
        
    return concat

# ä¸»å¤„ç†å‡½æ•°ï¼Œå°†è¢«UIè°ƒç”¨
def process_image(
    input_image,
    process_size=512,
    upscale=4,
    rec_type="recursive_multiscale",
    rec_num=4,
    prompt_type="vlm",
    align_method="nofix",
    user_prompt="",
    efficient_memory=True,
    mixed_precision="fp16",
    concat_layout="horizontal"
):
    # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨ç»“æœ
    temp_dir = "temp_results"
    os.makedirs(temp_dir, exist_ok=True)
    os.makedirs(os.path.join(temp_dir, 'per-sample'), exist_ok=True)
    os.makedirs(os.path.join(temp_dir, 'per-scale'), exist_ok=True)
    os.makedirs(os.path.join(temp_dir, 'recursive'), exist_ok=True)
    
    # ç”Ÿæˆå”¯ä¸€IDä½œä¸ºç»“æœæ ‡è¯†
    result_id = f"{int(time.time())}_{uuid.uuid4().hex[:8]}"
    
    # ä¿å­˜è¾“å…¥å›¾åƒ
    input_path = os.path.join(temp_dir, f"input_{result_id}.png")
    input_image.save(input_path)
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    class Args:
        def __init__(self):
            self.process_size = process_size
            self.upscale = upscale
            self.rec_type = rec_type
            self.rec_num = rec_num
            self.prompt_type = prompt_type
            self.align_method = align_method
            self.prompt = user_prompt
            self.efficient_memory = efficient_memory
            self.mixed_precision = mixed_precision
            self.output_dir = temp_dir
            self.input_image = input_path
            self.lora_path = "ckpt/SR_LoRA/model_20001.pkl"
            self.vae_path = "ckpt/SR_VAE/vae_encoder_20001.pt"
            self.pretrained_model_name_or_path = "stabilityai/stable-diffusion-3-medium-diffusers"
            self.ram_ft_path = "ckpt/DAPE/DAPE.pth"
            self.ram_path = "ckpt/RAM/ram_swin_large_14m.pth"
            self.save_prompts = True
            self.merge_and_unload_lora = False
            self.lora_rank = 4
            self.vae_decoder_tiled_size = 224
            self.vae_encoder_tiled_size = 1024
            self.latent_tiled_size = 96
            self.latent_tiled_overlap = 32
            self.seed = 42
    
    args = Args()
    
    global weight_dtype
    weight_dtype = torch.float16 if args.mixed_precision == "fp16" else torch.float32
    
    # åˆå§‹åŒ–æ¨¡å‹
    global model
    global model_test
    global vlm_model
    global vlm_processor
    global process_vision_info
    
    # åŠ è½½æ¨¡å‹ä»£ç ï¼ˆä¸åŸä»£ç ç›¸åŒï¼‰
    if rec_type not in ('nearest', 'bicubic'):
        if not args.efficient_memory:
            from osediff_sd3 import OSEDiff_SD3_TEST, SD3Euler
            model = SD3Euler()
            model.text_enc_1.to('cuda')
            model.text_enc_2.to('cuda')
            model.text_enc_3.to('cuda')
            model.transformer.to('cuda', dtype=torch.float32)
            model.vae.to('cuda', dtype=torch.float32)
            for p in [model.text_enc_1, model.text_enc_2, model.text_enc_3, model.transformer, model.vae]:
                p.requires_grad_(False)
            model_test = OSEDiff_SD3_TEST(args, model)
        else:
            from osediff_sd3 import OSEDiff_SD3_TEST_efficient, SD3Euler
            model = SD3Euler()
            model.transformer.to('cuda', dtype=torch.float32)
            model.vae.to('cuda', dtype=torch.float32)
            for p in [model.text_enc_1, model.text_enc_2, model.text_enc_3, model.transformer, model.vae]:
                p.requires_grad_(False)
            model_test = OSEDiff_SD3_TEST_efficient(args, model)
    
    # åŠ è½½DAPEæ¨¡å‹ï¼ˆå¦‚éœ€è¦ï¼‰
    DAPE = None
    if prompt_type == "dape":
        DAPE = ram(pretrained=args.ram_path,
                  pretrained_condition=args.ram_ft_path,
                  image_size=384,
                  vit='swin_l')
        DAPE.eval().to("cuda")
        DAPE = DAPE.to(dtype=weight_dtype)
    
    # åŠ è½½VLMæ¨¡å‹ï¼ˆå¦‚éœ€è¦ï¼‰
    if prompt_type == "vlm" and 'vlm_model' not in globals():
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        from qwen_vl_utils import process_vision_info
        
        vlm_model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
        print(f"Loading base VLM model: {vlm_model_name}")
        vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            vlm_model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        vlm_processor = AutoProcessor.from_pretrained(vlm_model_name)
    
    # å¤„ç†å›¾åƒ
    bname = f"result_{result_id}.png"
    rec_dir = os.path.join(args.output_dir, 'per-sample', result_id)
    os.makedirs(rec_dir, exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, 'per-scale', 'scale0'), exist_ok=True)
    
    # ç¬¬ä¸€å¼ å›¾åƒå¤„ç†
    first_image = resize_and_center_crop(input_image, args.process_size)
    first_image.save(f'{rec_dir}/0.png')
    first_image.save(os.path.join(args.output_dir, 'per-scale', 'scale0', bname))
    
    # é€’å½’å¤„ç†
    results = []
    results.append(first_image)
    
    # è®°å½•æ¯ä¸€æ­¥çš„æç¤ºè¯
    prompts = []
    prompts.append("åŸå§‹å›¾åƒ")
    
    for rec in range(args.rec_num):
        print(f'RECURSION: {rec}')
        os.makedirs(os.path.join(args.output_dir, 'per-scale', f'scale{rec+1}'), exist_ok=True)
        
        # æ ¹æ®é€’å½’ç±»å‹å¤„ç†å›¾åƒ
        if rec_type in ('nearest', 'bicubic', 'onestep'):
            # å¤„ç†ä»£ç ä¸åŸä»£ç ç›¸åŒ
            start_image_pil_path = f'{rec_dir}/0.png'
            start_image_pil = Image.open(start_image_pil_path).convert('RGB')
            rscale = pow(args.upscale, rec+1)
            w, h = start_image_pil.size
            new_w, new_h = w // rscale, h // rscale
            
            cropped_region = start_image_pil.crop(((w-new_w)//2, (h-new_h)//2, (w+new_w)//2, (h+new_h)//2))
            
            if rec_type == 'onestep':
                current_sr_input_image_pil = cropped_region.resize((w, h), Image.BICUBIC)
                prompt_image_path = f'{rec_dir}/0_input_for_{rec+1}.png'
                current_sr_input_image_pil.save(prompt_image_path)
            elif rec_type == 'bicubic':
                current_sr_input_image_pil = cropped_region.resize((w, h), Image.BICUBIC)
                current_sr_input_image_pil.save(f'{rec_dir}/{rec+1}.png')
                results.append(current_sr_input_image_pil)
                prompts.append(f"åŒä¸‰æ¬¡æ’å€¼ #{rec+1}")
                continue
            elif rec_type == 'nearest':
                current_sr_input_image_pil = cropped_region.resize((w, h), Image.NEAREST)
                current_sr_input_image_pil.save(f'{rec_dir}/{rec+1}.png')
                results.append(current_sr_input_image_pil)
                prompts.append(f"æœ€è¿‘é‚»æ’å€¼ #{rec+1}")
                continue
                
        elif rec_type == 'recursive':
            # å¤„ç†ä»£ç ä¸åŸä»£ç ç›¸åŒ
            prev_sr_output_path = f'{rec_dir}/{rec}.png'
            prev_sr_output_pil = Image.open(prev_sr_output_path).convert('RGB')
            rscale = args.upscale
            w, h = prev_sr_output_pil.size
            new_w, new_h = w // rscale, h // rscale
            cropped_region = prev_sr_output_pil.crop(((w-new_w)//2, (h-new_h)//2, (w+new_w)//2, (h+new_h)//2))
            current_sr_input_image_pil = cropped_region.resize((w, h), Image.BICUBIC)
            
            input_image_path = f'{rec_dir}/{rec+1}_input.png'
            current_sr_input_image_pil.save(input_image_path)
            prompt_image_path = input_image_path
            
        elif rec_type == 'recursive_multiscale':
            # å¤„ç†ä»£ç ä¸åŸä»£ç ç›¸åŒ
            prev_sr_output_path = f'{rec_dir}/{rec}.png'
            prev_sr_output_pil = Image.open(prev_sr_output_path).convert('RGB')
            rscale = args.upscale
            w, h = prev_sr_output_pil.size
            new_w, new_h = w // rscale, h // rscale
            cropped_region = prev_sr_output_pil.crop(((w-new_w)//2, (h-new_h)//2, (w+new_w)//2, (h+new_h)//2))
            current_sr_input_image_pil = cropped_region.resize((w, h), Image.BICUBIC)
            
            zoomed_image_path = f'{rec_dir}/{rec+1}_input.png'
            current_sr_input_image_pil.save(zoomed_image_path)
            prompt_image_path = [prev_sr_output_path, zoomed_image_path]
            
        # ç”Ÿæˆæç¤ºè¯
        validation_prompt, lq = get_validation_prompt(args, current_sr_input_image_pil, prompt_image_path, DAPE, vlm_model)
        print(f'TAG: {validation_prompt}')
        
        # ä¿å­˜æœ¬æ¬¡ä½¿ç”¨çš„æç¤ºè¯
        prompts.append(f"æ­¥éª¤ #{rec+1}: {validation_prompt[:50]}..." if len(validation_prompt) > 50 else f"æ­¥éª¤ #{rec+1}: {validation_prompt}")
        
        # è¶…åˆ†è¾¨ç‡å¤„ç†
        with torch.no_grad():
            lq = lq * 2 - 1
            
            if args.efficient_memory and model is not None:
                print("Ensuring SR model components are on CUDA for SR inference.")
                model.transformer.to('cuda', dtype=torch.float32)
                model.vae.to('cuda', dtype=torch.float32)
                
            output_image = model_test(lq, prompt=validation_prompt)
            output_image = torch.clamp(output_image[0].cpu(), -1.0, 1.0)
            output_pil = transforms.ToPILImage()(output_image * 0.5 + 0.5)
            if args.align_method == 'adain':
                output_pil = adain_color_fix(target=output_pil, source=current_sr_input_image_pil)
            elif args.align_method == 'wavelet':
                output_pil = wavelet_color_fix(target=output_pil, source=current_sr_input_image_pil)
                
        output_pil.save(f'{rec_dir}/{rec+1}.png')
        results.append(output_pil)
        
    # åˆ›å»ºæ‹¼æ¥å›¾åƒï¼Œä½¿ç”¨å¢å¼ºçš„æ‹¼æ¥å‡½æ•°
    concat = create_enhanced_concat(results, layout=concat_layout)
    concat_path = os.path.join(args.output_dir, 'recursive', bname)
    concat.save(concat_path)
    
    # è·å–æœ€ç»ˆå›¾åƒå°ºå¯¸ä¿¡æ¯
    final_image = results[-1]
    size_info = f"{final_image.width} x {final_image.height} åƒç´ "
    
    # è·å–æ¯ä¸€æ­¥çš„æ”¾å¤§åŒºåŸŸ
    zoom_regions = []
    
    # è®¡ç®—æ¯ä¸€æ­¥æ”¾å¤§çš„åŒºåŸŸï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    if rec_type in ('recursive', 'recursive_multiscale'):
        for i in range(rec_num):
            if i == 0:
                # ç¬¬ä¸€æ­¥çš„åŸå§‹å›¾åƒ
                orig_img = results[0].copy()
                w, h = orig_img.size
                rscale = args.upscale
                new_w, new_h = w // rscale, h // rscale
                left, top = (w - new_w) // 2, (h - new_h) // 2
                right, bottom = left + new_w, top + new_h
                
                # åœ¨åŸå›¾ä¸Šæ ‡è®°æ”¾å¤§åŒºåŸŸ
                from PIL import ImageDraw
                draw = ImageDraw.Draw(orig_img)
                draw.rectangle([left, top, right, bottom], outline="red", width=3)
                zoom_regions.append(orig_img)
            else:
                # åç»­æ­¥éª¤çš„å›¾åƒ
                orig_img = results[i].copy()
                w, h = orig_img.size
                rscale = args.upscale
                new_w, new_h = w // rscale, h // rscale
                left, top = (w - new_w) // 2, (h - new_h) // 2
                right, bottom = left + new_w, top + new_h
                
                # åœ¨å›¾åƒä¸Šæ ‡è®°æ”¾å¤§åŒºåŸŸ
                from PIL import ImageDraw
                draw = ImageDraw.Draw(orig_img)
                draw.rectangle([left, top, right, bottom], outline="red", width=3)
                zoom_regions.append(orig_img)
    
    # å°†æç¤ºè¯åˆ—è¡¨è½¬æ¢ä¸ºDataframeå¯æ¥å—çš„æ ¼å¼
    prompts_data = [[f"æ­¥éª¤ {i}", prompt] for i, prompt in enumerate(prompts)]
    
    # è¿”å›å¤„ç†ç»“æœ
    return results, concat, prompts_data, size_info, concat_path, zoom_regions

# Gradioç•Œé¢
def create_ui():
    with gr.Blocks(title="Chain-of-Zoom å›¾åƒè¶…åˆ†è¾¨ç‡å·¥å…·", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ” Chain-of-Zoom å›¾åƒè¶…åˆ†è¾¨ç‡å·¥å…·")
        gr.Markdown("ä½¿ç”¨AIæŠ€æœ¯é€æ­¥æ”¾å¤§å›¾åƒç»†èŠ‚ï¼Œå®ç°é«˜è´¨é‡çš„å›¾åƒè¶…åˆ†è¾¨ç‡")
        
        with gr.Row():
            with gr.Column(scale=1):
                input_image = gr.Image(label="è¾“å…¥å›¾åƒ", type="pil")
                
                with gr.Group():
                    gr.Markdown("### åŸºæœ¬å‚æ•°")
                    with gr.Row():
                        process_size = gr.Slider(label="å¤„ç†å°ºå¯¸", minimum=256, maximum=1024, value=512, step=64)
                        upscale = gr.Slider(label="æ¯æ¬¡æ”¾å¤§å€æ•°", minimum=2, maximum=8, value=4, step=1)
                    
                    with gr.Row():
                        rec_type = gr.Dropdown(
                            label="é€’å½’ç±»å‹", 
                            choices=["recursive_multiscale", "recursive", "onestep", "nearest", "bicubic"], 
                            value="recursive_multiscale",
                            info="multiscaleæ¨¡å¼å¯è·å¾—æœ€ä½³è´¨é‡"
                        )
                        rec_num = gr.Slider(label="é€’å½’æ¬¡æ•°", minimum=1, maximum=6, value=4, step=1)
                
                with gr.Group():
                    gr.Markdown("### é«˜çº§å‚æ•°")
                    with gr.Row():
                        prompt_type = gr.Dropdown(
                            label="æç¤ºè¯ç±»å‹", 
                            choices=["vlm", "dape", "null"], 
                            value="vlm",
                            info="VLMä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæç¤ºè¯"
                        )
                        align_method = gr.Dropdown(
                            label="é¢œè‰²å¯¹é½æ–¹æ³•", 
                            choices=["nofix", "wavelet", "adain"], 
                            value="nofix",
                            info="é¢œè‰²ä¿®æ­£ç®—æ³•"
                        )
                    
                    user_prompt = gr.Textbox(
                        label="è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼‰", 
                        placeholder="è¾“å…¥é¢å¤–çš„æç¤ºè¯ï¼Œç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆ",
                        info="é¢å¤–çš„æç¤ºè¯å°†ä¸è‡ªåŠ¨ç”Ÿæˆçš„æç¤ºè¯åˆå¹¶"
                    )
                    
                    with gr.Row():
                        efficient_memory = gr.Checkbox(label="é«˜æ•ˆå†…å­˜æ¨¡å¼", value=True, info="ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨")
                        mixed_precision = gr.Dropdown(label="ç²¾åº¦", choices=["fp16", "fp32"], value="fp16", info="fp16é€Ÿåº¦æ›´å¿«ï¼Œfp32è´¨é‡æ›´å¥½")
                
                with gr.Group():
                    gr.Markdown("### è¾“å‡ºé€‰é¡¹")
                    concat_layout = gr.Radio(
                        label="æ‹¼æ¥å¸ƒå±€", 
                        choices=["horizontal", "vertical", "grid"], 
                        value="horizontal",
                        info="é€‰æ‹©æœ€ç»ˆæ‹¼æ¥å›¾çš„å¸ƒå±€æ–¹å¼"
                    )
                
                process_btn = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
            
            with gr.Column(scale=2):
                with gr.Tabs():
                    with gr.TabItem("ç»“æœå±•ç¤º"):
                        size_info = gr.Textbox(label="æœ€ç»ˆå›¾åƒå°ºå¯¸", interactive=False)
                        final_output = gr.Image(label="æœ€ç»ˆæ‹¼æ¥ç»“æœ", type="pil", interactive=False)
                        download_btn = gr.Button("ä¸‹è½½å®Œæ•´æ‹¼æ¥å›¾", variant="secondary")
                        output_path = gr.Textbox(visible=False)  # éšè—è·¯å¾„
                        
                        with gr.Accordion("æ‰€æœ‰æ­¥éª¤ç»“æœ", open=False):
                            with gr.Row():
                                output_gallery = gr.Gallery(
                                    label="æ”¾å¤§è¿‡ç¨‹", 
                                    show_label=True,
                                    columns=5,
                                    object_fit="contain",
                                    height=300
                                )
                                # ä¿®å¤ï¼šç§»é™¤heightå‚æ•°
                                prompts_list = gr.Dataframe(
                                    headers=["æ­¥éª¤", "æç¤ºè¯"],
                                    datatype=["str", "str"],
                                    col_count=(2, "fixed"),
                                    interactive=False
                                )
                    
                    with gr.TabItem("æ”¾å¤§åŒºåŸŸå¯è§†åŒ–"):
                        zoom_regions_gallery = gr.Gallery(
                            label="æ¯æ­¥æ”¾å¤§åŒºåŸŸ", 
                            show_label=True,
                            columns=3,
                            object_fit="contain",
                            height=500
                        )
                
        # å¤„ç†å‡½æ•°è¿æ¥
        process_outputs = process_btn.click(
            fn=process_image,
            inputs=[
                input_image, process_size, upscale, rec_type, rec_num,
                prompt_type, align_method, user_prompt, efficient_memory, mixed_precision,
                concat_layout
            ],
            outputs=[
                output_gallery, final_output, prompts_list, size_info, output_path, zoom_regions_gallery
            ]
        )
        
        # ä¸‹è½½æŒ‰é’®åŠŸèƒ½
        def create_download_link(output_path):
            if not output_path:
                return None
            return output_path
            
        download_btn.click(
            fn=create_download_link,
            inputs=[output_path],
            outputs=[gr.File(label="ä¸‹è½½")]
        )
        
        # ç¤ºä¾‹å›¾åƒ
        gr.Examples(
            examples=[
                "samples/0393.png",
                "samples/0457.png",
                "samples/0479.png"
            ],
            inputs=input_image,
            outputs=[output_gallery, final_output, prompts_list, size_info, output_path, zoom_regions_gallery],
            fn=process_image,
            cache_examples=False,
        )
        
        gr.Markdown("""
        ### ä½¿ç”¨è¯´æ˜
        1. ä¸Šä¼ ä¸€å¼ å›¾åƒæˆ–ä½¿ç”¨ç¤ºä¾‹å›¾åƒ
        2. è°ƒæ•´å‚æ•°ï¼ˆå¤„ç†å°ºå¯¸ã€æ”¾å¤§å€æ•°ã€é€’å½’æ¬¡æ•°ç­‰ï¼‰
        3. ç‚¹å‡»"å¼€å§‹å¤„ç†"æŒ‰é’®
        4. åœ¨ç»“æœæ ‡ç­¾é¡µæŸ¥çœ‹å¤„ç†ç»“æœ
        5. ä½¿ç”¨"ä¸‹è½½å®Œæ•´æ‹¼æ¥å›¾"æŒ‰é’®ä¿å­˜ç»“æœ
        
        ### å‚æ•°è§£é‡Š
        - **å¤„ç†å°ºå¯¸**ï¼šè¾“å…¥å›¾åƒçš„å¤„ç†å°ºå¯¸ï¼Œæ›´å¤§çš„å°ºå¯¸éœ€è¦æ›´å¤šå†…å­˜
        - **æ¯æ¬¡æ”¾å¤§å€æ•°**ï¼šæ¯æ­¥æ”¾å¤§çš„å€æ•°ï¼Œé€šå¸¸ä¸º4å€
        - **é€’å½’ç±»å‹**ï¼š
          - recursive_multiscale: å¤šå°ºåº¦é€’å½’æ”¾å¤§ï¼ˆæœ€ä½³è´¨é‡ï¼‰
          - recursive: ç®€å•é€’å½’æ”¾å¤§
          - onestep: ä¸€æ¬¡æ€§æ”¾å¤§
          - nearest/bicubic: ä½¿ç”¨ç®€å•æ’å€¼ç®—æ³•
        - **é€’å½’æ¬¡æ•°**ï¼šæ‰§è¡Œæ”¾å¤§çš„æ¬¡æ•°ï¼Œæ›´å¤šæ¬¡æ•°å¯ä»¥æ”¾å¤§æ›´å°çš„ç»†èŠ‚
        - **æç¤ºè¯ç±»å‹**ï¼š
          - vlm: ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæç¤ºè¯ï¼ˆæ¨èï¼‰
          - dape: ä½¿ç”¨DAPEæ¨¡å‹ç”Ÿæˆæç¤ºè¯
          - null: ä¸ä½¿ç”¨æç¤ºè¯
        - **é¢œè‰²å¯¹é½**ï¼šå¯¹ç”Ÿæˆå›¾åƒçš„é¢œè‰²è¿›è¡Œä¿®æ­£
        """)
        
    return demo

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    ui = create_ui()
    ui.launch(share=False, server_name="0.0.0.0")
